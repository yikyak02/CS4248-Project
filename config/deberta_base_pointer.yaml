seed: 42
processed_dir: data/processed_dataset
val_processed_dir: data/processed_dev 

# Backbone (same family, smaller)
encoder_name: microsoft/deberta-v3-large

# Windowing used by preprocessing + predict (smaller = faster)
max_length: 384
doc_stride: 64
max_answer_len: 30

# Head (unchanged logic)
head_type: pointer
topk_start: 5

# Training (anti-overfitting configuration)
epochs: 40
train_batch_size: 16          # Smaller batches for better generalization
grad_accum_steps: 3           # Effective batch size = 48
lr: 2e-5                      # Lower learning rate
weight_decay: 0.1             # Stronger L2 regularization
warmup_ratio: 0.1             # More warmup steps
max_grad_norm: 1.0
amp: true                     # only engages on CUDA

label_smoothing: 0.1          # Increased from 0.05
dropout: 0.2                  # Added dropout regularization
scheduler: cosine             # Cosine annealing better for overfitting
ema: true
ema_decay: 0.9995             # Slightly higher decay

# Early stopping (more patient)
early_stopping: true
patience: 5                   # Increased patience
val_interval: 1

# IO / logging
output_dir: outputs/deberta_v3_small_pointer_fast
log_interval: 25
save_every_steps: 0

# Dataloader
num_workers: 8               # safe on macOS; increase on Linux if desired
